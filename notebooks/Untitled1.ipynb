{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bb8b513-a960-412f-a7d1-0041bdaa6c0f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3092c338-aa0b-4aa9-87cb-85ac0583facf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load image from the IAM database\n",
    "url = 'aligned.png'\n",
    "'''\n",
    "image = Image.open(url).convert('L')\n",
    "threshold = 127\n",
    "image = image.point(lambda p: 255 if p > threshold else 0)\n",
    "image = image.convert('1')\n",
    "image\n",
    "'''\n",
    "img = cv2.imread(url, cv2.IMREAD_GRAYSCALE)\n",
    "#otsu_threshold, image = cv2.threshold(\n",
    "    #image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU,\n",
    "#)\n",
    "\n",
    "# do adaptive threshold on gray image\n",
    "img = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 199, 15)\n",
    "\n",
    "# make background of input white where thresh is white\n",
    "#result = image.copy()\n",
    "\n",
    "# write results to disk\n",
    "cv2.imwrite(\"binary-aligned.png\", img)\n",
    "#cv2.imwrite(\"math_diagram_processed.jpg\", result)\n",
    "\n",
    "# display it\n",
    "#cv2.imshow(\"THRESHOLD\", thresh)\n",
    "#cv2.imshow(\"RESULT\", result)\n",
    "#cv2.waitKey(0)\n",
    "\n",
    "#plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3cfec8f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3736, 2048)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05379e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3736, 2048)\n",
      "387\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start by finding all of the connected components (white blobs in your image).\n",
    "# 'im' needs to be grayscale and 8bit.\n",
    "#img_uint8 = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "img_uint8 = ((255-img)).astype(np.uint8)\n",
    "print(img_uint8.shape)\n",
    "\n",
    "num_components, img_with_separated_components, stats, _ = cv2.connectedComponentsWithStats(img_uint8)\n",
    "# im_with_separated_blobs is an image where each detected blob has a different pixel value ranging from 1 to nb_blobs - 1.\n",
    "# The background pixels have value 0.\n",
    "print(num_components)\n",
    "# 'stats' (and the silenced output 'centroids') provides information about the blobs. See the docs for more information. \n",
    "# Here, we're interested only in the size of the blobs :\n",
    "sizes = stats[:, cv2.CC_STAT_AREA]\n",
    "# You can also directly index the column with '-1' instead of 'cv2.CC_STAT_AREA' as it's the last column.\n",
    "\n",
    "# A small gotcha is that the background is considered as a blob, and so its stats are included in the stats vector at position 0.\n",
    "\n",
    "# minimum size of particles we want to keep (number of pixels).\n",
    "# here, it's a fixed value, but you can set it as you want, eg the mean of the sizes or whatever.\n",
    "min_size = 159  \n",
    "\n",
    "# create empty output image with will contain only the biggest composents\n",
    "img = np.zeros_like(img)\n",
    "\n",
    "# for every component in the image, keep it only if it's above min_size.\n",
    "# we start at 1 to avoid considering the background\n",
    "for component in range(1, num_components):\n",
    "    if sizes[component] >= min_size:\n",
    "        img[img_with_separated_components == component] = 255\n",
    "\n",
    "\n",
    "cv2.imwrite(\"denoised.png\", img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec2d080a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel = np.ones((3, 3), np.uint8)\n",
    "#img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
    "\n",
    "kernel = np.ones((2, 2), np.uint8)\n",
    "#img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
    "\n",
    "kernel = np.ones((2, 1), np.uint8)\n",
    "#img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
    "\n",
    "kernel = np.ones((1, 2), np.uint8)\n",
    "#img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
    "\n",
    "kernel = np.ones((2, 1), np.uint8)\n",
    "img = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel, iterations=3)\n",
    "\n",
    "kernl = np.ones((2, 2), np.uint8)\n",
    "#img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel, iterations=3)\n",
    "\n",
    "cv2.imwrite(\"denoised.png\", img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "828efb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278\n",
      "[(45, 1168, 489, 436, 131), (47, 150, 490, 430, 132), (48, 588, 491, 435, 132), (55, 149, 629, 431, 133), (56, 588, 629, 435, 134), (58, 1168, 628, 437, 134), (64, 148, 768, 432, 134), (65, 1168, 769, 437, 134), (66, 588, 770, 436, 132), (71, 148, 907, 433, 136), (72, 588, 910, 437, 134), (74, 1168, 910, 437, 136), (83, 148, 1048, 434, 137), (84, 590, 1050, 435, 138), (86, 1168, 1053, 437, 136), (97, 149, 1189, 434, 138), (98, 590, 1193, 435, 137), (100, 1168, 1196, 436, 136), (101, 1611, 1197, 427, 140), (108, 149, 1331, 434, 139), (109, 590, 1336, 435, 137), (111, 1168, 1339, 434, 136), (112, 1609, 1340, 429, 139), (115, 149, 1474, 434, 139), (116, 590, 1479, 434, 135), (118, 1166, 1481, 434, 137), (119, 1606, 1484, 432, 137), (127, 150, 1617, 433, 138), (128, 591, 1621, 433, 135), (130, 1166, 1624, 432, 136), (131, 1605, 1627, 431, 135), (139, 150, 1759, 433, 137), (140, 590, 1763, 434, 135), (142, 1166, 1765, 431, 136), (143, 1605, 1768, 431, 136), (154, 151, 1901, 432, 136), (155, 590, 1904, 433, 135), (157, 1166, 1906, 432, 136), (158, 1606, 1909, 430, 137), (165, 151, 2042, 431, 137), (166, 590, 2045, 432, 136), (168, 1165, 2048, 433, 136), (169, 1606, 2050, 429, 138), (172, 151, 2183, 431, 136), (173, 590, 2186, 431, 136), (175, 1165, 2189, 433, 137), (176, 1606, 2193, 429, 137), (179, 151, 2325, 431, 135), (180, 589, 2327, 432, 136), (182, 1165, 2330, 434, 138), (183, 1606, 2334, 429, 138), (189, 150, 2467, 432, 135), (190, 589, 2468, 433, 137), (192, 1165, 2472, 435, 139), (193, 1607, 2476, 428, 139), (196, 150, 2609, 431, 136), (197, 589, 2610, 432, 138), (199, 1166, 2614, 435, 139), (200, 1608, 2619, 426, 139), (210, 148, 2752, 433, 136), (211, 588, 2753, 434, 138), (213, 1166, 2757, 437, 140), (214, 1610, 2762, 423, 138), (220, 147, 2896, 433, 135), (221, 588, 2896, 434, 138), (223, 1166, 2900, 437, 140), (224, 1611, 2905, 422, 137), (231, 146, 3039, 434, 136), (232, 588, 3039, 435, 139), (234, 1166, 3044, 437, 140), (235, 1611, 3049, 421, 135), (242, 146, 3182, 434, 139), (243, 588, 3184, 436, 139), (245, 1167, 3188, 436, 139), (246, 1611, 3192, 420, 135)]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m40\u001b[39m, \u001b[38;5;241m60\u001b[39m):\n\u001b[1;32m     33\u001b[0m     temp\u001b[38;5;241m.\u001b[39mappend(move_boxes[i])\n\u001b[0;32m---> 34\u001b[0m     temp\u001b[38;5;241m.\u001b[39mappend(move_boxes[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m20\u001b[39m])\n\u001b[1;32m     35\u001b[0m move_boxes \u001b[38;5;241m=\u001b[39m temp\n\u001b[1;32m     37\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "num_components, img_with_separated_components, stats, _ = cv2.connectedComponentsWithStats(img)\n",
    "\n",
    "move_boxes = []\n",
    "print(num_components)\n",
    "\n",
    "for component in range(1, num_components):\n",
    "    x = stats[component, cv2.CC_STAT_LEFT]\n",
    "    y = stats[component, cv2.CC_STAT_TOP]\n",
    "    w = stats[component, cv2.CC_STAT_WIDTH]\n",
    "    h = stats[component, cv2.CC_STAT_HEIGHT]\n",
    "    \n",
    "    if 3. < w/h < 4.:\n",
    "        move_boxes.append((component, x, y, w, h))\n",
    "        \n",
    "\n",
    "print(move_boxes)\n",
    "\n",
    "# sort boxes into correct order\n",
    "\n",
    "move_boxes.sort(key=lambda x: x[1])\n",
    "move_boxes[:20] = sorted(move_boxes[:20], key=lambda x: x[2])\n",
    "move_boxes[20:40] = sorted(move_boxes[20:40], key=lambda x: x[2])\n",
    "move_boxes[40:60] = sorted(move_boxes[40:60], key=lambda x: x[2])\n",
    "move_boxes[60:80] = sorted(move_boxes[60:80], key=lambda x: x[2])\n",
    "\n",
    "test = img.copy()\n",
    "\n",
    "temp = []\n",
    "for i in range(20):\n",
    "    temp.append(move_boxes[i])\n",
    "    temp.append(move_boxes[i+20])\n",
    "for i in range(40, 60):\n",
    "    temp.append(move_boxes[i])\n",
    "    temp.append(move_boxes[i+20])\n",
    "move_boxes = temp\n",
    "\n",
    "i = 0\n",
    "for move_box in move_boxes:\n",
    "    component, x, y, w, h = move_box\n",
    "\n",
    "    cv2.putText(test,str(i), \n",
    "    (x, y+h), \n",
    "    cv2.FONT_HERSHEY_SIMPLEX, \n",
    "    1,\n",
    "    127,\n",
    "    1,\n",
    "    2)\n",
    "    i += 1\n",
    "\n",
    "cv2.imwrite(\"denoised.png\", test)\n",
    "\n",
    "\n",
    "#cv2.imwrite(\"denoised.png\", test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08ff1058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ideas to deal with 'g', since it often spills over into the cell below\n",
    "# 1. floodfill cells only from the TOP — this will filter out the bottom part of a stray 'g' from the cell above\n",
    "# 2. crop out the bottom by set # of pixels as usual — this may turn our 'g' into a '9', so our NN should treat '9's as 'g's\n",
    "# 3. maybe do an area-based noise reduction after cropping the bottom?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a23f030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pixel length to crop out from each side (ideally the maximum width of a grid line)\n",
    "_, x, y, w, h = move_boxes[60]\n",
    "\n",
    "crop = 5\n",
    "\n",
    "test_box = 255 - img[y+crop:y+h-crop, x+crop:x+w-crop]   \n",
    "cv2.imwrite(\"box.png\", test_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdd54a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIX THIS!!!!!!!!!!!\n",
    "# Seems like this only pads by 1 pixel instead of 2?\n",
    "\n",
    "def format_to_mnist(img):\n",
    "\n",
    "    img = cv2.GaussianBlur(img,(5,5),1)\n",
    "\n",
    "    img_h, img_w = img.shape\n",
    "    dim_size_max = max(img.shape)\n",
    "\n",
    "    if dim_size_max == img_w:\n",
    "        im_h = (24 * img_h) // img_w\n",
    "        if im_h <= 0 or img_w <= 0:\n",
    "            print(\"Invalid Image Dimention: \", im_h, img_w, img_h)\n",
    "        tmp_img = cv2.resize(img, (24,im_h),0,0,cv2.INTER_CUBIC)\n",
    "    else:\n",
    "        im_w = (24 * img_w) // img_h\n",
    "        if im_w <= 0 or img_h <= 0:\n",
    "            print(\"Invalid Image Dimention: \", im_w, img_w, img_h)\n",
    "        tmp_img = cv2.resize(img, (im_w, 24),0,0,cv2.INTER_CUBIC)\n",
    "\n",
    "    out_img = np.zeros((28, 28), dtype=np.ubyte)\n",
    "\n",
    "    nb_h, nb_w = out_img.shape\n",
    "    na_h, na_w = tmp_img.shape\n",
    "    y_min = (nb_w) // 2 - (na_w // 2)\n",
    "    y_max = y_min + na_w\n",
    "    x_min = (nb_h) // 2 - (na_h // 2)\n",
    "    x_max = x_min + na_h\n",
    "\n",
    "    out_img[x_min:x_max, y_min:y_max] = tmp_img\n",
    "\n",
    "    return out_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "133c179a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the contours\n",
    "contours, hierarchy = cv2.findContours(test_box, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "contours = sorted(contours, key=lambda x:cv2.boundingRect(x)[0])\n",
    "\n",
    "# Loop over each contour and save it as a new image\n",
    "for i, contour in enumerate(contours):\n",
    "    # Create a mask for the current contour\n",
    "    mask = np.zeros_like(test_box)\n",
    "    cv2.drawContours(mask, [contour], -1, 255, -1)\n",
    "    \n",
    "    # Extract the contour region from the original image using the mask\n",
    "    contour_region = cv2.bitwise_and(test_box, test_box, mask=mask)\n",
    "    \n",
    "    # Find the bounding box of the contour to crop the region\n",
    "    x, y, w, h = cv2.boundingRect(contour)\n",
    "    cropped_contour = contour_region[y:y+h, x:x+w]\n",
    "\n",
    "    # convert to MNIST format\n",
    "    cropped_contour = format_to_mnist(cropped_contour)\n",
    "    \n",
    "    # Save the extracted contour region as a new image\n",
    "    cv2.imwrite(f'mnist_images/contour_{i}.png', cropped_contour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85de294b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
